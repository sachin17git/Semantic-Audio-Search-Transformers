{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model architecture](arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import boto3\n",
    "import torch\n",
    "from PyPDF2 import PdfFileReader\n",
    "from collections import namedtuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers.pipelines import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import python modules to perform AWS some operations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import AwsOperations\n",
    "import VoiceRecorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up audio configurations and AWS clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VoiceConfig = namedtuple(\"VoiceConfig\", \"seconds fs\")\n",
    "v_config = VoiceConfig(5, 16000)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "transcribe = boto3.client(\"transcribe\", region_name=\"us-east-2\")\n",
    "\n",
    "aws_op = AwsOperations.AwsOp(s3, transcribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to record and upload audio to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_rec_start(fname):\n",
    "    VoiceRecorder.recorder(sec=v_config.seconds, fs=v_config.fs)\n",
    "    aws_op.upload_audio_s3(filename=fname)\n",
    "    q = aws_op.transcribe_job(filename=os.path.basename(fname))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 bucket Clean up function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_rec_clean_up(fname):\n",
    "    aws_op.delete_audio_s3(os.path.basename(fname).split(\".\")[0])\n",
    "    aws_op.delete_job(\"S2T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading machine learning model called transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader():\n",
    "    print(\"Loading models.\")\n",
    "    emb = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model_name = \"phiyodr/bart-large-finetuned-squad2\"\n",
    "    print(\"All models loaded.\")\n",
    "    nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "    print(\"Pipeline created.\")\n",
    "    return emb, nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read, clean and create a corpus file out of a pdf/txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_create_corpus(file_name):\n",
    "    corpus_temp = []\n",
    "    if pathlib.Path(file_name).suffix == \".pdf\":\n",
    "        txt_from_pdf = []\n",
    "        sent_adder = lambda x: x.replace(\"\\n\", \"\").strip()\n",
    "        reader = PdfFileReader(file_name)\n",
    "        n_pages = reader.getNumPages()\n",
    "        print(f\"File format : PDF\\nPages detected: {n_pages}\")\n",
    "\n",
    "        for p in range(n_pages):\n",
    "            page = reader.getPage(p)\n",
    "            pdf_text_page = page.extractText()\n",
    "            txt_from_pdf.append(pdf_text_page.strip())\n",
    "\n",
    "        for para in txt_from_pdf:\n",
    "            sent = para.split(\".\")\n",
    "            sent = [s.strip() for s in sent if s.__len__() > 5]\n",
    "            corpus_temp = corpus_temp + list(map(sent_adder, sent))\n",
    "\n",
    "    elif pathlib.Path(file_name).suffix == \".txt\":\n",
    "        sent_adder = lambda x: x.strip()\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = file.readlines()\n",
    "            data = [d.strip() for d in data]\n",
    "\n",
    "        for para in data:\n",
    "            sent = para.split(\".\")\n",
    "            sent = [s.strip() for s in sent if s.__len__() > 5]\n",
    "            corpus_temp = corpus_temp + list(map(sent_adder, sent))\n",
    "\n",
    "    return corpus_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a semantic search function using ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(qs, top):\n",
    "    text = []\n",
    "    for qry in qs:\n",
    "        query_embedding = embedder.encode(qry, convert_to_tensor=True)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top)\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            text.append(corpus[idx])\n",
    "        con = '. '.join(text)\n",
    "        return con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline for question answering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(question, e_context, nlp):\n",
    "    inputs = {\n",
    "        'question': question,\n",
    "        'context': e_context\n",
    "    }\n",
    "    return nlp(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File format : PDF\n",
      "Pages detected: 2\n",
      "Loading models.\n",
      "All models loaded.\n",
      "Pipeline created.\n",
      "Corpus (number of sentences) : 43\n",
      "Please speak (search query) : \n",
      "Recording started\n",
      "Recording complete\n",
      "Record saved\n",
      "s3://aws-audio/rec\n",
      "Not ready yet...\n",
      "Not ready yet...\n",
      "Not ready yet...\n",
      "\n",
      "\n",
      "Search query : What is Covid?\n",
      "Result : a pandemic of respiratory illness\n",
      "Context:\n",
      "\n",
      "COVID-19 is severe and has caused millions of deaths around the world. COVID-19 is a pandemic of respiratory illness caused by SARS-CoV-2 virus, which was discovered in 2019. The COVID  data was gathered from CDC (Centers for Disease Control and Prevention), where they have a separate section for COVID-19 death data and resources. The spread of SARS-CoV-2, the causative agent of COVID-19, has resulted in an unprecedented global public health and economic crisis. When people with COVID exhale, they can discharge particles and droplets of respiratory fluids into the air\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_inp = str(input(\"Enter the file name: \"))\n",
    "    filename_document = os.path.join(os.getcwd(), file_inp)\n",
    "    try:\n",
    "        corpus = read_create_corpus(filename_document)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        exit(1)\n",
    "    embedder, NLP = model_loader()\n",
    "    print(\"Corpus (number of sentences) :\", corpus.__len__())\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    top_k = min(5, len(corpus))\n",
    "    filename = os.path.join(os.getcwd(), \"rec.wav\")\n",
    "    print(\"Please speak (search query) : \")\n",
    "    inp = 'Y'\n",
    "\n",
    "    while inp not in [\"N\", \"n\"]:\n",
    "        query = aws_rec_start(filename)\n",
    "        print(\"\\n\")\n",
    "        queries = [query.strip()]\n",
    "        context = search(queries, top_k)\n",
    "        result = qa(queries[0], context, NLP)\n",
    "        print(\"Search query :\", queries[0])\n",
    "        print(f\"Result : {result['answer']}\")\n",
    "        print(f\"Context:\\n\")\n",
    "        print(context)\n",
    "        print(\"\\n\")\n",
    "        aws_rec_clean_up(filename)\n",
    "        inp = str(input(\"Continue searching? [Y/N]: \"))\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f7cd4a032357cee79ce7059b7758f9c5aa995e9cd3eea18c8a9dd6a7286ed82"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ub_dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
